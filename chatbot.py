# -*- coding: utf-8 -*-
"""Chatbot-Deployment_no_outputs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ONxtyE1qGvQ_NnkP6G5tX3u1N4dkryIj

# Descargar modelo en español
"""

# !python -m spacy download es_core_news_md
# !python -m spacy link es_core_news_md es --force

"""# Descarga y carga de los stopwords"""

import nltk
from nltk.corpus import stopwords

nltk.download("stopwords")
stopwords = stopwords.words("spanish")

"""# Limpieza de los stopwords en un texto"""

def remove_stopwords(text):
  text_filtered = ""
  for word in text.split():
    if word not in stopwords:
      text_filtered += word + " "
  return text_filtered

"""#Obtenemos las relaciones de dependencia"""

import spacy
nlp = spacy.load('es_core_news_md')

def generate_bigrams(sentence):
  bigram_list = []
  for token in sentence:
    if(token.dep_ != 'ROOT' and token.dep_ != 'punct'):
      bigram_list.append(token.head.text+'_'+token.text)
  
  example = sentence.text
  if(len(bigram_list) > 0): example = ' '.join(bigram_list)
  return example

"""# Transformar un texto a un vector de números"""

from sklearn.feature_extraction.text import CountVectorizer

#Cargamos la biblioteca para obtener nuestro Bag-Of-Words
count_vectorizer = CountVectorizer()

"""# Creamos un pequeño dataset de intents asociados a sentencias"""

dataset = """
*saludos
hola
cómo estás
cómo vás
cómo te va
qué tal

*búsqueda_restaurantes
estoy buscando un lugar para comer
quiero almorzar
busco un lugar para cenar
muestrame restaurantes chinos
busca restaurantes

*afirmación
si
ok
vale
me parece bien
de acuerdo, gracias

*despedida
chau
hasta pronto
nos vemos
hasta la próxima
hasta otra oportunidad
"""

"""# Entrenamos un modelo con nuestro dataset

## Métodos para crear el dataset
"""

def filter_intents_and_their_examples(dataset, intent_character = "*"):
  filter_list = list(filter(None, dataset.split("\n")))

  intents_examples = {}
  intent = ""
  for element in filter_list:
    if element[0] == intent_character:
      intent = element[1:]
    else:
      if intent not in intents_examples:
        intents_examples[intent] = [generate_bigrams(nlp(element))]
      else:
        intents_examples[intent].append(generate_bigrams(nlp(element)))
  return intents_examples

def transform_examples_in_vectors(intents_examples, count_vectorizer):
  texts_list = list()
  
  for key, values in intents_examples.items():
    texts_list.extend(values)

  count_vectorizer.fit(texts_list)
  
  intents_vector_examples = {}

  for key, values in intents_examples.items():
    intents_vector_examples[key] = count_vectorizer.transform(values).toarray()
  
  return intents_vector_examples

def create_x_and_y(intents_vector_examples):
  x = []
  y = []
  for label, vector_examples in intents_vector_examples.items():
    x.extend(vector_examples)
    y.extend([label] * len(vector_examples))
  
  return x, y

#Obteniendo un diccionario de intents con sus sentencias
intents_examples = filter_intents_and_their_examples(dataset)

intents_examples

#Transformando las sentencias a vectores en el diccionario
intents_vector_examples = transform_examples_in_vectors(intents_examples, count_vectorizer)

intents_vector_examples

#Obteniendo los nombres de los intents
intents = list(intents_vector_examples.keys())
intents

"""## Creando el dataset"""

#Obteniendo x,y para entrenar un modelo de Machine Learning
x, y = create_x_and_y(intents_vector_examples)

y[0]

"""## Entrenando el modelo de Machine Learning"""

from sklearn.tree import DecisionTreeClassifier

decision_tree_classifier = DecisionTreeClassifier()
decision_tree_classifier.fit(x, y)

#Evaluacion del accuracy
accuracy = decision_tree_classifier.score(x, y)
accuracy

"""# Creamos un módulo que use nuestro modelo para predecir los **Intents**

## Método que representa nuestro módulo
"""

def predict_intent(sentence, count_vectorizer, model):
  vector = count_vectorizer.transform([ generate_bigrams(nlp(sentence)) ])
  
  intent = model.predict(vector)
  return intent[0]

"""# Creamos un pequeño dataset de utterances asociados a sentencias"""

dataset = """
*saludos
hola
qué bueno verte
qué me cuentas
qué hay de nuevo
qué onda

*recomendación_restaurante
Bien, yo te recomiendo el 7 sopas. Te parece bien ?
Que tal el Chifa Oso Panda, suena bien ?
El Shimaya Ramen es una buena opción, te animas ?

*afirmación
excelente
bueno
de nada

*despedida
chau
nos vemos
adios
"""

"""## Métodos para crear el dataset"""

def filter_utterances_and_their_examples(dataset, utterance_character = "*"):
  filter_list = list(filter(None, dataset.split("\n")))

  utterances_examples = {}
  utterance = ""
  for element in filter_list:
    if element[0] == utterance_character:
      utterance = element[1:]
    else:
      if utterance not in utterances_examples:
        utterances_examples[utterance] = [element]
      else:
        utterances_examples[utterance].append(element)
  return utterances_examples

"""## Creando el dataset"""

#Obteniendo un diccionario de utterances con sus sentencias
utterances_examples = filter_utterances_and_their_examples(dataset)
utterances_examples

"""## Definimos las reglas intent - utterance"""

rules = {
    "saludos": "saludos",
    "búsqueda_restaurantes": "recomendación_restaurante",
    "afirmación": "afirmación",
    "despedida": "despedida"
}

"""# Creamos un módulo que genere lo que el chatbot va responder

## Método que representa nuestro módulo
"""

from random import choice

def generate_answer(utterance, utterances_examples):
  answers = utterances_examples[utterance]
  answer = choice(answers)
  return answer

"""# Creamos el módulo para conversar con el chatbot

## Método que representa nuestro módulo
"""

def return_answer(sentence, count_vectorizer, model, rules, utterances_examples):
  intent = predict_intent(sentence, count_vectorizer, decision_tree_classifier)
  
  utterance = rules[intent]
  
  answer = generate_answer(utterance, utterances_examples)

  return answer, intent

"""## Serializar el CountVectorizer y DecisionTreeClassifier"""

import pickle

pickle.dump(count_vectorizer, open("count_vectorizer.pickle", "wb"))
pickle.dump(decision_tree_classifier, open("decision_tree_classifier.pickle", "wb"))
pickle.dump(rules, open("rules.pickle", "wb"))
pickle.dump(utterances_examples, open("utterances_examples.pickle", "wb"))